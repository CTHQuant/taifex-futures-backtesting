{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b21a52-4006-48ee-823e-5986bfcbef41",
   "metadata": {},
   "source": [
    "## 台指日盤當沖策略(Gridsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a27be37a-3365-4fb2-ba98-4f265dcfdbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "總參數組合數：256\n",
      "已從 output_path 載入 256 個已處理的參數組合\n",
      "前 5 個已處理的參數組合：[(1.1, 0.9, 1.5, 1.3, 0.7, 0.5, 1.5, 4.0), (1.1, 0.8, 1.5, 1.3, 0.7, 0.6, 1.5, 3.5), (1.2, 0.9, 1.5, 1.3, 0.8, 0.5, 1.5, 3.5), (1.2, 0.8, 1.5, 1.3, 0.7, 0.5, 1.5, 3.5), (1.2, 0.9, 1.5, 1.3, 0.7, 0.6, 2.0, 3.5)]\n",
      "剩餘未處理的參數組合數：0\n",
      "所有參數組合均已處理，無需繼續執行。\n",
      "最佳參數組合：\n",
      "BSR_THRESHOLD_LONG        1.1\n",
      "BSR_THRESHOLD_SHORT       0.9\n",
      "sl_k                      4.0\n",
      "tp_k                      1.5\n",
      "vr_high                   1.6\n",
      "vr_mid                    0.8\n",
      "vr_mid_high               1.2\n",
      "vr_mid_low                0.5\n",
      "trade_count             115.0\n",
      "final_cumpnl           1733.0\n",
      "Name: 108, dtype: float64\n",
      "\n",
      "所有參數組合結果：\n",
      "     BSR_THRESHOLD_LONG  BSR_THRESHOLD_SHORT  sl_k  tp_k  vr_high  vr_mid  \\\n",
      "108                 1.1                  0.9   4.0   1.5      1.6     0.8   \n",
      "109                 1.1                  0.9   4.0   1.5      1.6     0.8   \n",
      "101                 1.1                  0.9   4.0   1.5      1.5     0.8   \n",
      "100                 1.1                  0.9   4.0   1.5      1.5     0.8   \n",
      "65                  1.1                  0.9   3.5   1.5      1.5     0.7   \n",
      "..                  ...                  ...   ...   ...      ...     ...   \n",
      "30                  1.1                  0.8   3.5   2.0      1.6     0.8   \n",
      "52                  1.1                  0.8   4.0   2.0      1.5     0.8   \n",
      "53                  1.1                  0.8   4.0   2.0      1.5     0.8   \n",
      "49                  1.1                  0.8   4.0   2.0      1.5     0.7   \n",
      "48                  1.1                  0.8   4.0   2.0      1.5     0.7   \n",
      "\n",
      "     vr_mid_high  vr_mid_low  trade_count  final_cumpnl  \n",
      "108          1.2         0.5          115          1733  \n",
      "109          1.2         0.6          115          1733  \n",
      "101          1.2         0.6          115          1676  \n",
      "100          1.2         0.5          115          1676  \n",
      "65           1.2         0.6          115          1662  \n",
      "..           ...         ...          ...           ...  \n",
      "30           1.3         0.5           58          -404  \n",
      "52           1.2         0.5           58          -412  \n",
      "53           1.2         0.6           58          -412  \n",
      "49           1.2         0.6           58          -418  \n",
      "48           1.2         0.5           58          -418  \n",
      "\n",
      "[256 rows x 10 columns]\n",
      "\n",
      "GridSearch 結果已儲存至 output_path\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta, time\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import gc\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# 設定區\n",
    "# =============================================================================\n",
    "START_DATE = \"2024-01-01\"\n",
    "END_DATE   = \"2024-12-31\"\n",
    "SAVE_DIR   = r\"C:\\Users\\USER\\Desktop\\雲端同步\\Shioaji\\Backtesting\"\n",
    "PATTERN    = os.path.join(SAVE_DIR, \"stock_kbars_*.parquet\")\n",
    "\n",
    "stock_number = 5\n",
    "\n",
    "STOCKS_TO_FETCH = [\"2330\", \"2454\", \"2317\", \"2881\", \"2412\", \"2382\", \"2308\", \"2882\", \"2891\", \"3711\"]\n",
    "\n",
    "bsr_long_grid   = np.round(np.arange(1.1, 1.21, 0.1), 2)\n",
    "bsr_short_grid  = np.round(np.arange(0.8, 0.91, 0.1), 2)\n",
    "vr_high_grid    = np.round(np.arange(1.5, 1.61, 0.1), 2)\n",
    "vr_midh_grid    = np.round(np.arange(1.2, 1.31, 0.1), 2)\n",
    "vr_mid_grid     = np.round(np.arange(0.7, 0.81, 0.1), 2)\n",
    "vr_midlow_grid  = np.round(np.arange(0.5, 0.61, 0.1), 2)\n",
    "tp_k_grid       = np.arange(1.5, 2.1, 0.5)\n",
    "sl_k_grid       = np.arange(3.5, 4.1, 0.5)\n",
    "\n",
    "param_grid = {\n",
    "    'BSR_THRESHOLD_LONG': bsr_long_grid,\n",
    "    'BSR_THRESHOLD_SHORT': bsr_short_grid,\n",
    "    'vr_high': vr_high_grid,\n",
    "    'vr_mid_high': vr_midh_grid,\n",
    "    'vr_mid': vr_mid_grid,\n",
    "    'vr_mid_low': vr_midlow_grid,\n",
    "    'tp_k': tp_k_grid,\n",
    "    'sl_k': sl_k_grid\n",
    "}\n",
    "\n",
    "start_dt = datetime.strptime(START_DATE, \"%Y-%m-%d\").date()\n",
    "end_dt   = datetime.strptime(END_DATE, \"%Y-%m-%d\").date()\n",
    "\n",
    "# =============================================================================\n",
    "# 1. 取最近 21 個交易日（含當日）→ 計算「前 20 日不含當日」的振幅統計\n",
    "# =============================================================================\n",
    "dates_df_full = (\n",
    "    pl.scan_parquet(os.path.join(SAVE_DIR, \"txf_kbars.parquet\"))\n",
    "    .with_columns(pl.col(\"ts\").dt.date().alias(\"date\"))\n",
    "    .select(\"date\")\n",
    "    .filter(pl.col(\"date\") <= end_dt)\n",
    "    .unique().sort(\"date\")\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "analysis_dates_df = dates_df_full.filter(\n",
    "    (pl.col(\"date\") >= start_dt) & (pl.col(\"date\") <= end_dt)\n",
    ")\n",
    "\n",
    "pre_window_dates = (\n",
    "    dates_df_full\n",
    "      .filter(pl.col(\"date\") < start_dt)\n",
    "      .tail(40)\n",
    ")\n",
    "\n",
    "dates_df = pl.concat([pre_window_dates, analysis_dates_df])\n",
    "\n",
    "start_trading   = dates_df[\"date\"][0]\n",
    "analysis_dates  = analysis_dates_df[\"date\"]\n",
    "\n",
    "kbars_lazy = (\n",
    "    pl.scan_parquet(os.path.join(SAVE_DIR, \"txf_kbars.parquet\"))\n",
    "    .rename({\"Open\":\"open\",\"High\":\"high\",\"Low\":\"low\",\"Close\":\"close\",\"Volume\":\"volume\"})\n",
    "    .with_columns([\n",
    "        pl.col(\"ts\").cast(pl.Datetime(\"us\")), \n",
    "        pl.col(\"ts\").dt.date().alias(\"date\"),\n",
    "        pl.col(\"ts\").dt.hour().alias(\"hour\"),\n",
    "        pl.col(\"ts\").dt.minute().alias(\"minute\"),\n",
    "    ])\n",
    "    .filter(\n",
    "        (pl.col(\"date\") >= start_trading) &\n",
    "        (pl.col(\"date\") <= end_dt) &\n",
    "        (\n",
    "            ((pl.col(\"hour\") == 8)  & (pl.col(\"minute\") >= 45)) |\n",
    "            ((pl.col(\"hour\") > 8)   & (pl.col(\"hour\") < 13)) |\n",
    "            ((pl.col(\"hour\") == 13) & (pl.col(\"minute\") <= 45))\n",
    "        )\n",
    "    )\n",
    "    .drop([\"hour\", \"minute\"])\n",
    "    .sort([\"date\",\"ts\"])\n",
    ")\n",
    "\n",
    "minute_close_lazy = (\n",
    "    kbars_lazy\n",
    "    .with_columns(pl.col(\"ts\").dt.truncate(\"1m\").alias(\"minute_ts\"))\n",
    "    .group_by([\"date\", \"minute_ts\"])\n",
    "    .agg(pl.col(\"close\").last().alias(\"close\"))\n",
    "    .rename({\"minute_ts\": \"ts\"})\n",
    "    .sort([\"date\",\"ts\"])\n",
    "    .with_columns([\n",
    "        pl.col(\"close\").forward_fill().over(\"date\")\n",
    "    ])\n",
    "    .lazy()\n",
    ")\n",
    "\n",
    "daily_range_lazy = (\n",
    "    kbars_lazy\n",
    "    .group_by(\"date\")\n",
    "    .agg([\n",
    "        pl.col(\"high\").max().alias(\"high_max\"),\n",
    "        pl.col(\"low\").min().alias(\"low_min\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        (pl.col(\"high_max\") - pl.col(\"low_min\")).alias(\"amplitude\")\n",
    "    ])\n",
    "    .sort(\"date\")\n",
    "    .with_columns([\n",
    "        pl.col(\"amplitude\").shift(1).alias(\"amp_prev\"),\n",
    "        pl.col(\"amplitude\").shift(1).rolling_max(window_size=20).alias(\"max\"),\n",
    "        pl.col(\"amplitude\").shift(1).rolling_mean(window_size=20).alias(\"avg\"),\n",
    "        pl.col(\"amplitude\").shift(1).rolling_min(window_size=20).alias(\"min\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.col(\"amp_prev\").rank(method=\"average\").over(pl.lit(1)).alias(\"rank\")\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.when(pl.col(\"rank\") > 10)\n",
    "          .then(None)\n",
    "          .otherwise(pl.col(\"amp_prev\"))\n",
    "          .rolling_mean(window_size=20)\n",
    "          .alias(\"top\"),\n",
    "        pl.when(pl.col(\"rank\") <= (pl.len().over(pl.lit(1)) - 10))\n",
    "          .then(pl.col(\"amp_prev\"))\n",
    "          .otherwise(None)\n",
    "          .rolling_mean(window_size=20)\n",
    "          .alias(\"bot\")\n",
    "    ])\n",
    "    .select([\"date\", \"amp_prev\", \"max\", \"avg\", \"min\", \"top\", \"bot\", \"amplitude\"])\n",
    ")\n",
    "\n",
    "daily_range_df = daily_range_lazy.collect().to_pandas()\n",
    "daily_range_df[\"top\"] = daily_range_df[\"amp_prev\"].rolling(20, min_periods=1) \\\n",
    "    .apply(lambda x: pd.Series(x).nlargest(10).mean(), raw=False)\n",
    "daily_range_df[\"bot\"] = daily_range_df[\"amp_prev\"].rolling(20, min_periods=1) \\\n",
    "    .apply(lambda x: pd.Series(x).nsmallest(10).mean(), raw=False)\n",
    "daily_range_df = daily_range_df.drop(columns=[\"rank\"], errors=\"ignore\")\n",
    "\n",
    "daily_range_lazy = (\n",
    "    pl.from_pandas(daily_range_df)\n",
    "      .with_columns([\n",
    "          pl.col(\"date\").cast(pl.Date),\n",
    "          pl.col(\"max\"),\n",
    "          pl.col(\"avg\"),\n",
    "          pl.col(\"min\"),\n",
    "          pl.col(\"top\"),\n",
    "          pl.col(\"bot\"),\n",
    "      ])\n",
    "      .lazy()\n",
    ")\n",
    "\n",
    "del daily_range_df\n",
    "gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# 2. 取最近 21 個交易日的 ticks → 每分鐘累積 vs 過去 20 日 KBARS 平均累積 → volume_ratio\n",
    "# =============================================================================\n",
    "kbars_min_cum = (\n",
    "    kbars_lazy\n",
    "    .with_columns(pl.col(\"ts\").dt.truncate(\"1m\").alias(\"minute_ts\"))\n",
    "    .group_by([\"date\", \"minute_ts\"])\n",
    "    .agg(pl.col(\"volume\").sum().alias(\"min_vol_k\"))\n",
    "    .sort([\"date\", \"minute_ts\"])\n",
    "    .with_columns([\n",
    "        pl.col(\"min_vol_k\").cum_sum().over(\"date\").alias(\"cum_vol_k\"),\n",
    "        pl.col(\"minute_ts\").dt.strftime(\"%H:%M\").alias(\"minute_str\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "kbars_avg_cum = (\n",
    "    kbars_min_cum\n",
    "    .with_columns(\n",
    "        pl.col(\"cum_vol_k\")\n",
    "          .shift(1)\n",
    "          .rolling_mean(20)\n",
    "          .over(\"minute_str\")\n",
    "          .alias(\"avg_cum_vol\")\n",
    "    )\n",
    "    .group_by(\"minute_str\")\n",
    "    .agg(pl.col(\"avg_cum_vol\").last())\n",
    ")\n",
    "\n",
    "volume_ratio_lazy = (\n",
    "    kbars_min_cum\n",
    "    .join(kbars_avg_cum, on=\"minute_str\", how=\"left\")\n",
    "    .with_columns([\n",
    "        (pl.col(\"cum_vol_k\") / pl.col(\"avg_cum_vol\")).alias(\"volume_ratio\")\n",
    "    ])\n",
    "    .with_columns(\n",
    "        pl.col(\"volume_ratio\").fill_null(0).forward_fill().over(\"date\")\n",
    "    )\n",
    "    .select([\n",
    "        pl.col(\"date\"),\n",
    "        pl.col(\"minute_ts\"),\n",
    "        pl.col(\"cum_vol_k\"),\n",
    "        pl.col(\"avg_cum_vol\"),\n",
    "        pl.col(\"volume_ratio\")\n",
    "    ])\n",
    "    .rename({\n",
    "        \"minute_ts\": \"ts\",\n",
    "        \"cum_vol_k\": \"cum_vol\"\n",
    "    })\n",
    "    .lazy()\n",
    ")\n",
    "\n",
    "del kbars_min_cum, kbars_avg_cum\n",
    "gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# 3. 每分鐘內外盤成交比 bsr\n",
    "# =============================================================================\n",
    "ticks_base_lazy = (\n",
    "    pl.scan_parquet(os.path.join(SAVE_DIR, \"txf_ticks.parquet\"))\n",
    "    .with_columns([\n",
    "        pl.col(\"ts\").cast(pl.Datetime(\"us\")).alias(\"ts\"),\n",
    "        pl.col(\"ts\").dt.date().alias(\"date\"),\n",
    "        pl.col(\"volume\"),\n",
    "        pl.col(\"tick_type\")\n",
    "    ])\n",
    "    .filter(\n",
    "        pl.col(\"ts\").dt.time().is_between(time(8,45), time(13,45)) &\n",
    "        pl.col(\"tick_type\").is_in([1,2])\n",
    "    )\n",
    "    .sort([\"date\",\"ts\"])\n",
    "    .with_columns([\n",
    "        pl.arange(0, pl.len()).over(\"date\").alias(\"row_idx\")\n",
    "    ])\n",
    "    .filter(pl.col(\"row_idx\") > 0)\n",
    "    .drop(\"row_idx\")\n",
    "    .lazy()\n",
    ")\n",
    "\n",
    "daily_bsr_lazy = (\n",
    "    ticks_base_lazy\n",
    "    .with_columns([\n",
    "        pl.when(pl.col(\"tick_type\")==1)\n",
    "          .then(pl.col(\"volume\")).otherwise(0)\n",
    "          .cum_sum().over(\"date\").alias(\"outer_cum\"),\n",
    "        pl.when(pl.col(\"tick_type\")==2)\n",
    "          .then(pl.col(\"volume\")).otherwise(0)\n",
    "          .cum_sum().over(\"date\").alias(\"inner_cum\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.when(pl.col(\"inner_cum\")==0)\n",
    "          .then(None)\n",
    "          .otherwise(pl.col(\"outer_cum\")/pl.col(\"inner_cum\"))\n",
    "          .alias(\"bsr\")\n",
    "    ])\n",
    "    .with_columns(pl.col(\"ts\").dt.truncate(\"1m\").alias(\"minute_ts\"))\n",
    "    .group_by([\"date\",\"minute_ts\"])\n",
    "    .agg(pl.col(\"bsr\").last())\n",
    "    .sort([\"date\",\"minute_ts\"])\n",
    "    .rename({\"minute_ts\":\"ts\"})\n",
    "    .with_columns(pl.col(\"bsr\").forward_fill().over(\"date\"))\n",
    ")\n",
    "\n",
    "del ticks_base_lazy\n",
    "gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. 計算權值股紅K比例 + TSMC 判斷\n",
    "# =============================================================================\n",
    "parquet_paths = glob.glob(PATTERN)\n",
    "stock_lfs = []\n",
    "for path in parquet_paths:\n",
    "    sid = os.path.basename(path).split(\"_\")[-1].replace(\".parquet\", \"\")\n",
    "    lf = (\n",
    "        pl.read_parquet(path).lazy()\n",
    "        .rename({\"Open\": \"open\", \"Close\": \"close\"})\n",
    "        .with_columns([\n",
    "            pl.col(\"ts\").cast(pl.Datetime),\n",
    "            pl.col(\"ts\").dt.date().alias(\"date\"),\n",
    "            pl.lit(sid).alias(\"sid\")\n",
    "        ])\n",
    "    )\n",
    "    stock_lfs.append(lf)\n",
    "\n",
    "stock_lazy = (\n",
    "    pl.concat(stock_lfs)\n",
    "    .filter(pl.col(\"sid\").is_in(STOCKS_TO_FETCH))\n",
    ")\n",
    "\n",
    "opening_price = (\n",
    "    stock_lazy\n",
    "    .filter(pl.col(\"ts\").dt.time() >= datetime.strptime(\"09:00\", \"%H:%M\").time())\n",
    "    .sort([\"sid\", \"date\", \"ts\"])\n",
    "    .group_by([\"sid\", \"date\"])\n",
    "    .agg(pl.col(\"open\").first().alias(\"day_open\"))\n",
    ")\n",
    "\n",
    "minute_close = (\n",
    "    stock_lazy\n",
    "    .with_columns(pl.col(\"ts\").dt.truncate(\"1m\").alias(\"minute_ts\"))\n",
    "    .group_by([\"sid\",\"date\",\"minute_ts\"])\n",
    "    .agg(pl.col(\"close\").last().alias(\"minute_close\"))\n",
    ")\n",
    "\n",
    "minute_status_lazy = (\n",
    "    minute_close\n",
    "    .join(opening_price, on=[\"sid\",\"date\"])\n",
    "    .with_columns([\n",
    "        pl.col(\"minute_ts\").alias(\"ts\"),\n",
    "        (pl.col(\"minute_close\") > pl.col(\"day_open\")).alias(\"is_red\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "tsmc_status_lazy = (\n",
    "    minute_status_lazy\n",
    "    .filter(pl.col(\"sid\") == \"2330\")\n",
    "    .select([\"date\",\"ts\",\"is_red\"])\n",
    "    .rename({\"is_red\":\"tsmc_red\"})\n",
    ")\n",
    "\n",
    "summary_lazy = (\n",
    "    minute_status_lazy\n",
    "    .group_by([\"date\",\"ts\"])\n",
    "    .agg(pl.col(\"is_red\").sum().alias(\"is_red\"))\n",
    "    .join(tsmc_status_lazy, on=[\"date\",\"ts\"], how=\"left\")\n",
    "    .with_columns([\n",
    "        pl.col(\"is_red\").forward_fill().over(\"date\"),\n",
    "        pl.col(\"tsmc_red\").forward_fill().over(\"date\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "del stock_lfs, stock_lazy, opening_price, minute_close, minute_status_lazy, tsmc_status_lazy\n",
    "gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# 5. 合併所有指標\n",
    "# =============================================================================\n",
    "joined_all = (\n",
    "    volume_ratio_lazy\n",
    "    .join(daily_range_lazy, on=\"date\", how=\"left\")\n",
    "    .join(daily_bsr_lazy, on=[\"date\",\"ts\"], how=\"left\")\n",
    "    .join(minute_close_lazy, on=[\"date\",\"ts\"], how=\"left\")\n",
    "    .join(summary_lazy, on=[\"date\",\"ts\"], how=\"left\")\n",
    "    .with_columns(pl.col(\"ts\").cast(pl.Datetime(\"us\")))\n",
    "    .filter(pl.col(\"ts\").dt.time().is_between(time(8,45), time(13,45)))\n",
    ")\n",
    "\n",
    "float_cols = [\n",
    "    \"cum_vol\",\"avg_cum_vol\",\"volume_ratio\",\n",
    "    \"amp_prev\",\"max\",\"avg\",\"min\",\"top\",\"bot\",\"amplitude\",\n",
    "    \"bsr\",\"close\"\n",
    "]\n",
    "int_cols = [\"is_red\"]\n",
    "bool_cols = [\"tsmc_red\"]\n",
    "\n",
    "joined_filled = (\n",
    "    joined_all\n",
    "    .with_columns([\n",
    "        pl.col(c)\n",
    "          .forward_fill().over(\"date\")\n",
    "          .fill_null(0.0)\n",
    "          .alias(c)\n",
    "        for c in float_cols\n",
    "    ] + [\n",
    "        pl.col(c)\n",
    "          .forward_fill().over(\"date\")\n",
    "          .fill_null(0)\n",
    "          .alias(c)\n",
    "        for c in int_cols\n",
    "    ] + [\n",
    "        pl.col(c)\n",
    "          .forward_fill().over(\"date\")\n",
    "          .fill_null(False)\n",
    "          .alias(c)\n",
    "        for c in bool_cols\n",
    "    ])\n",
    ")\n",
    "\n",
    "joined_lazy = joined_filled.filter(\n",
    "    pl.col(\"date\").is_between(\n",
    "        datetime.strptime(START_DATE, \"%Y-%m-%d\").date(),\n",
    "        datetime.strptime(END_DATE, \"%Y-%m-%d\").date()\n",
    "    )\n",
    ")\n",
    "\n",
    "del joined_all, joined_filled\n",
    "gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# 6. 準備 future ticks 資料（用於動態 TP/SL 出場檢查）\n",
    "# =============================================================================\n",
    "future_base = (\n",
    "    pl.scan_parquet(f\"{SAVE_DIR}/txf_ticks.parquet\")\n",
    "    .with_columns([\n",
    "        pl.col(\"ts\").cast(pl.Datetime(\"us\")).alias(\"future_time\"),\n",
    "        pl.col(\"ts\").dt.date().alias(\"date\"),\n",
    "    ])\n",
    "    .filter(\n",
    "        pl.col(\"date\").is_between(start_trading, end_dt) &\n",
    "        (pl.col(\"future_time\").dt.time() >= time(8, 45)) &\n",
    "        (pl.col(\"future_time\").dt.time() <= time(13, 45))\n",
    "    )\n",
    "    .select([\"future_time\", \"close\", \"date\"])\n",
    "    .rename({\"close\": \"future_close\"})\n",
    ")\n",
    "\n",
    "future_lazy = (\n",
    "    future_base\n",
    "    .join_asof(\n",
    "        joined_lazy\n",
    "          .with_columns(pl.col(\"ts\").alias(\"minute_ts\"))\n",
    "          .select([\n",
    "              pl.col(\"date\"),\n",
    "              pl.col(\"minute_ts\"),\n",
    "              pl.col(\"bsr\"), pl.col(\"volume_ratio\"),\n",
    "              pl.col(\"max\"), pl.col(\"top\"), pl.col(\"avg\"),\n",
    "              pl.col(\"bot\"), pl.col(\"min\")\n",
    "          ])\n",
    "          .sort([\"date\", \"minute_ts\"]),\n",
    "        left_on=\"future_time\",\n",
    "        right_on=\"minute_ts\",\n",
    "        by=\"date\",\n",
    "        strategy=\"backward\",\n",
    "        tolerance=timedelta(minutes=2)\n",
    "    )\n",
    "    .sort([\"date\", \"future_time\"])\n",
    "    .with_columns(\n",
    "        pl.col(\"volume_ratio\").forward_fill().over(\"date\")\n",
    "    )\n",
    "    .with_columns(pl.col(\"future_close\").round(0).cast(pl.Int32))\n",
    ")\n",
    "\n",
    "future_df = future_lazy.collect().to_pandas()\n",
    "future_df = future_df[future_df[\"future_close\"].notna()]\n",
    "\n",
    "del future_base, future_lazy\n",
    "gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# 7. GridSearch 主迴圈 - 分塊執行\n",
    "# =============================================================================\n",
    "CHUNK_SIZE = 10\n",
    "output_path = os.path.join(SAVE_DIR, \"grid_search_results.csv\")\n",
    "\n",
    "header_written = os.path.exists(output_path)\n",
    "\n",
    "param_list = list(ParameterGrid(param_grid))\n",
    "total_params = len(param_list)\n",
    "print(f\"總參數組合數：{total_params}\")\n",
    "\n",
    "processed_params = set()\n",
    "if header_written:\n",
    "    try:\n",
    "        existing_results = pd.read_csv(output_path)\n",
    "        param_cols = list(param_grid.keys())\n",
    "        missing_cols = [col for col in param_cols if col not in existing_results.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"CSV 檔案缺少欄位：{missing_cols}，將從頭開始\")\n",
    "            processed_params = set()\n",
    "        else:\n",
    "            for _, row in existing_results[param_cols].iterrows():\n",
    "                try:\n",
    "                    param_tuple = tuple(\n",
    "                        round(float(val), 2) if isinstance(val, (float, np.floating, str)) and pd.notna(val) else val\n",
    "                        for val in row\n",
    "                    )\n",
    "                    processed_params.add(param_tuple)\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    print(f\"無法處理 CSV 行：{row}，錯誤：{e}\")\n",
    "                    continue\n",
    "            print(f\"已從 output_path 載入 {len(processed_params)} 個已處理的參數組合\")\n",
    "            print(f\"前 5 個已處理的參數組合：{list(processed_params)[:5]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"無法讀取現有結果檔案：{e}，將從頭開始\")\n",
    "\n",
    "unprocessed_params = []\n",
    "for params in param_list:\n",
    "    param_tuple = tuple(\n",
    "        round(float(params[key]), 2) if isinstance(params[key], (float, np.floating)) else params[key]\n",
    "        for key in param_cols\n",
    "    )\n",
    "    if param_tuple not in processed_params:\n",
    "        unprocessed_params.append(params)\n",
    "\n",
    "total_unprocessed = len(unprocessed_params)\n",
    "print(f\"剩餘未處理的參數組合數：{total_unprocessed}\")\n",
    "\n",
    "if total_unprocessed == 0:\n",
    "    print(\"所有參數組合均已處理，無需繼續執行。\")\n",
    "else:\n",
    "    for chunk_start in range(0, total_unprocessed, CHUNK_SIZE):\n",
    "        chunk_end = min(chunk_start + CHUNK_SIZE, total_unprocessed)\n",
    "        chunk_params = unprocessed_params[chunk_start:chunk_end]\n",
    "        chunk_results = []\n",
    "    \n",
    "        print(f\"正在處理參數塊 {chunk_start // CHUNK_SIZE + 1} ({chunk_start} 到 {chunk_end})...\")\n",
    "    \n",
    "        for params in chunk_params:\n",
    "            BSR_THRESHOLD_LONG = params['BSR_THRESHOLD_LONG']\n",
    "            BSR_THRESHOLD_SHORT = params['BSR_THRESHOLD_SHORT']\n",
    "            VOLUME_RATIO_LEVELS = {\n",
    "                \"high\": params['vr_high'],\n",
    "                \"mid_high\": params['vr_mid_high'],\n",
    "                \"mid\": params['vr_mid'],\n",
    "                \"mid_low\": params['vr_mid_low'],\n",
    "            }\n",
    "            TP_K = params['tp_k']\n",
    "            SL_K = params['sl_k']\n",
    "    \n",
    "            kbars_seg = (\n",
    "                kbars_lazy\n",
    "                .filter(pl.col(\"ts\").dt.time() >= time(8, 45))\n",
    "                .with_columns([\n",
    "                    pl.col(\"high\").cum_max().over(\"date\").alias(\"seg_high\"),\n",
    "                    pl.col(\"low\").cum_min().over(\"date\").alias(\"seg_low\"),\n",
    "                ])\n",
    "                .select([\"date\", \"ts\", \"seg_high\", \"seg_low\"])\n",
    "            )\n",
    "    \n",
    "            entry_candidates = (\n",
    "                joined_lazy\n",
    "                .filter(~pl.col(\"ts\").dt.time().is_between(time(8, 45), time(9, 0)))\n",
    "                .filter(pl.col(\"bsr\").is_not_null())\n",
    "                .filter(\n",
    "                    (\n",
    "                        (pl.col(\"tsmc_red\") == True) &\n",
    "                        (pl.col(\"is_red\") > stock_number) &\n",
    "                        (pl.col(\"bsr\") > BSR_THRESHOLD_LONG)\n",
    "                    ) |\n",
    "                    (\n",
    "                        (pl.col(\"tsmc_red\") == False) &\n",
    "                        (pl.col(\"is_red\") < stock_number) &\n",
    "                        (pl.col(\"bsr\") < BSR_THRESHOLD_SHORT)\n",
    "                    )\n",
    "                )\n",
    "                .with_columns([\n",
    "                    pl.col(\"ts\").alias(\"entry_time\"),\n",
    "                    pl.col(\"close\").alias(\"entry\"),\n",
    "                    pl.when(pl.col(\"tsmc_red\") & (pl.col(\"is_red\") > stock_number))\n",
    "                    .then(pl.lit(\"long\"))\n",
    "                    .otherwise(pl.lit(\"short\"))\n",
    "                    .alias(\"side\"),\n",
    "                    pl.col(\"volume_ratio\"),\n",
    "                    pl.col(\"bsr\"),\n",
    "                ])\n",
    "                .sort(\"entry_time\")\n",
    "            )\n",
    "    \n",
    "            entry_lazy = (\n",
    "                entry_candidates\n",
    "                .with_columns([\n",
    "                    pl.when(pl.col(\"side\") == \"long\")\n",
    "                    .then(pl.col(\"bsr\"))\n",
    "                    .otherwise(None)\n",
    "                    .cum_max()\n",
    "                    .shift(1)\n",
    "                    .alias(\"prev_max_long\"),\n",
    "                    pl.when(pl.col(\"side\") == \"short\")\n",
    "                    .then(pl.col(\"bsr\"))\n",
    "                    .otherwise(None)\n",
    "                    .cum_min()\n",
    "                    .shift(1)\n",
    "                    .alias(\"prev_min_short\"),\n",
    "                ])\n",
    "                .filter(\n",
    "                    (\n",
    "                        (pl.col(\"side\") == \"long\") &\n",
    "                        (\n",
    "                            pl.col(\"prev_max_long\").is_null() |\n",
    "                            (pl.col(\"bsr\") > pl.col(\"prev_max_long\"))\n",
    "                        )\n",
    "                    ) |\n",
    "                    (\n",
    "                        (pl.col(\"side\") == \"short\") &\n",
    "                        (\n",
    "                            pl.col(\"prev_min_short\").is_null() |\n",
    "                            (pl.col(\"bsr\") < pl.col(\"prev_min_short\"))\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                .drop([\"prev_max_long\", \"prev_min_short\"])\n",
    "                .join_asof(\n",
    "                    kbars_seg.sort([\"date\", \"ts\"]),\n",
    "                    left_on=\"entry_time\",\n",
    "                    right_on=\"ts\",\n",
    "                    by=\"date\",\n",
    "                    strategy=\"backward\"\n",
    "                )\n",
    "                .with_columns(\n",
    "                    (pl.col(\"seg_high\") - pl.col(\"seg_low\")).alias(\"seg_amp\")\n",
    "                )\n",
    "                .filter(pl.col(\"seg_amp\") <= pl.col(\"avg\"))\n",
    "                .with_columns(\n",
    "                    pl.arange(0, pl.len()).over(\"date\").alias(\"seq\")\n",
    "                )\n",
    "                .filter(pl.col(\"seq\") < 3)\n",
    "                .drop(\"seq\")\n",
    "            )\n",
    "    \n",
    "            entry_df = entry_lazy.collect().to_pandas()\n",
    "    \n",
    "            del kbars_seg, entry_candidates, entry_lazy\n",
    "            gc.collect()\n",
    "    \n",
    "            exit_records = []\n",
    "    \n",
    "            def compute_tp_sl_offset(vol, max_, top, avg, bot, min_):\n",
    "                if vol > VOLUME_RATIO_LEVELS[\"high\"]:\n",
    "                    return (max_ + top) / TP_K, (max_ + top) / SL_K\n",
    "                elif vol > VOLUME_RATIO_LEVELS[\"mid_high\"]:\n",
    "                    return (top + avg) / TP_K, (top + avg) / SL_K\n",
    "                elif vol > VOLUME_RATIO_LEVELS[\"mid\"]:\n",
    "                    return avg , avg / 2\n",
    "                elif vol > VOLUME_RATIO_LEVELS[\"mid_low\"]:\n",
    "                    return (avg + bot) / TP_K, (avg + bot) / SL_K\n",
    "                else:\n",
    "                    return (bot + min_) / TP_K, (bot + min_) / SL_K\n",
    "    \n",
    "            for _, row in entry_df.iterrows():\n",
    "                last_vol = None\n",
    "                date = row[\"date\"]\n",
    "                entry_time = row[\"entry_time\"]\n",
    "                entry_price = row[\"entry\"]\n",
    "                side = row[\"side\"]\n",
    "                cutoff_time = pd.Timestamp(f\"{date} 13:30:00\")\n",
    "                top_amp = row[\"top\"]\n",
    "                seg_high = row[\"seg_high\"]\n",
    "                seg_low = row[\"seg_low\"]\n",
    "                seg_amp = row[\"seg_amp\"]\n",
    "    \n",
    "                fut_ticks = future_df[\n",
    "                    (future_df[\"date\"] == date) &\n",
    "                    (future_df[\"future_time\"] > entry_time) &\n",
    "                    (future_df[\"future_time\"] <= cutoff_time)\n",
    "                ]\n",
    "    \n",
    "                exit_time, exit_price, exit_type = None, None, \"Close\"\n",
    "                tp_pts, sl_pts = None, None\n",
    "                long_tp, long_sl, short_tp, short_sl = None, None, None, None\n",
    "    \n",
    "                if fut_ticks.empty:\n",
    "                    continue\n",
    "                else:\n",
    "                    for _, tick in fut_ticks.iterrows():\n",
    "                        vol = tick[\"volume_ratio\"]\n",
    "                        if pd.isna(vol):\n",
    "                            vol = last_vol\n",
    "                        else:\n",
    "                            last_vol = vol\n",
    "    \n",
    "                        if vol is None:\n",
    "                            continue\n",
    "    \n",
    "                        tp_offset, sl_offset = compute_tp_sl_offset(\n",
    "                            vol, tick[\"max\"], tick[\"top\"],\n",
    "                            tick[\"avg\"], tick[\"bot\"], tick[\"min\"]\n",
    "                        )\n",
    "                        if pd.isna(tp_offset) or pd.isna(sl_offset):\n",
    "                            continue\n",
    "                        if seg_amp >= (top_amp / 2):\n",
    "                            tp_offset /= 2\n",
    "                            sl_offset /= 2\n",
    "    \n",
    "                        tp_pts, sl_pts = int(round(tp_offset)), int(round(sl_offset))\n",
    "    \n",
    "                        if side == \"long\":\n",
    "                            tp_price = entry_price + tp_offset\n",
    "                            sl_price = entry_price - sl_offset\n",
    "                            long_tp, long_sl = int(round(tp_price)), int(round(sl_price))\n",
    "                            if tick[\"future_close\"] >= tp_price:\n",
    "                                exit_time, exit_price, exit_type = tick[\"future_time\"], tp_price, \"TP\"\n",
    "                                break\n",
    "                            elif tick[\"future_close\"] <= sl_price:\n",
    "                                exit_time, exit_price, exit_type = tick[\"future_time\"], sl_price, \"SL\"\n",
    "                                break\n",
    "                        else:\n",
    "                            tp_price = entry_price - tp_offset\n",
    "                            sl_price = entry_price + sl_offset\n",
    "                            short_tp, short_sl = int(round(tp_price)), int(round(sl_price))\n",
    "                            if tick[\"future_close\"] <= tp_price:\n",
    "                                exit_time, exit_price, exit_type = tick[\"future_time\"], tp_price, \"TP\"\n",
    "                                break\n",
    "                            elif tick[\"future_close\"] >= sl_price:\n",
    "                                exit_time, exit_price, exit_type = tick[\"future_time\"], sl_price, \"SL\"\n",
    "                                break\n",
    "    \n",
    "                    if exit_time is None:\n",
    "                        last_tick = fut_ticks.iloc[-1]\n",
    "                        tp_offset, sl_offset = compute_tp_sl_offset(\n",
    "                            last_tick[\"volume_ratio\"], last_tick[\"max\"], last_tick[\"top\"],\n",
    "                            last_tick[\"avg\"], last_tick[\"bot\"], last_tick[\"min\"]\n",
    "                        )\n",
    "                        if pd.isna(tp_offset) or pd.isna(sl_offset):\n",
    "                            continue\n",
    "                        tp_pts, sl_pts = int(round(tp_offset)), int(round(sl_offset))\n",
    "                        exit_time = last_tick[\"future_time\"]\n",
    "                        exit_price = last_tick[\"future_close\"]\n",
    "                        if side == \"long\":\n",
    "                            long_tp = int(round(entry_price + tp_offset))\n",
    "                            long_sl = int(round(entry_price - sl_offset))\n",
    "                        else:\n",
    "                            short_tp = int(round(entry_price - tp_offset))\n",
    "                            short_sl = int(round(entry_price + sl_offset))\n",
    "    \n",
    "                pnl = exit_price - entry_price if side == \"long\" else entry_price - exit_price\n",
    "    \n",
    "                exit_records.append({\n",
    "                    \"entry_time\": entry_time,\n",
    "                    \"exit_time\": exit_time,\n",
    "                    \"type\": exit_type,\n",
    "                    \"date\": date,\n",
    "                    \"side\": side,\n",
    "                    \"hold\": entry_price,\n",
    "                    \"long_tp\": long_tp if side == \"long\" else None,\n",
    "                    \"long_sl\": long_sl if side == \"long\" else None,\n",
    "                    \"short_tp\": short_tp if side == \"short\" else None,\n",
    "                    \"short_sl\": short_sl if side == \"short\" else None,\n",
    "                    \"pnl\": int(round(pnl)),\n",
    "                    \"tp_pts\": tp_pts,\n",
    "                    \"sl_pts\": sl_pts,\n",
    "                    \"vol%\": round(row[\"volume_ratio\"], 2),\n",
    "                    \"bsr\": round(row[\"bsr\"], 2),\n",
    "                })\n",
    "    \n",
    "            del entry_df, fut_ticks\n",
    "            gc.collect()\n",
    "    \n",
    "            if not exit_records:\n",
    "                chunk_results.append({\n",
    "                    **params,\n",
    "                    'trade_count': 0,\n",
    "                    'final_cumpnl': 0\n",
    "                })\n",
    "                continue\n",
    "    \n",
    "            exit_df = pd.DataFrame(exit_records)\n",
    "            exit_df[\"cumpnl\"] = exit_df[\"pnl\"].cumsum()\n",
    "    \n",
    "            range_df = daily_range_lazy.collect().to_pandas()\n",
    "            range_df = range_df[[\"date\", \"max\", \"top\", \"avg\", \"bot\", \"min\", \"amplitude\"]]\n",
    "            exit_df = exit_df.merge(range_df, on=\"date\", how=\"left\")\n",
    "    \n",
    "            red_df = (\n",
    "                summary_lazy\n",
    "                .select([\"date\", \"ts\", \"is_red\"])\n",
    "                .collect()\n",
    "                .rename({\"ts\": \"entry_time\"})\n",
    "                .to_pandas()\n",
    "            )\n",
    "    \n",
    "            exit_df[\"entry_time_str\"] = pd.to_datetime(exit_df[\"entry_time\"]).dt.strftime(\"%H:%M\")\n",
    "            red_df[\"entry_time_str\"] = pd.to_datetime(red_df[\"entry_time\"]).dt.strftime(\"%H:%M\")\n",
    "            exit_df = exit_df.merge(red_df[[\"date\", \"entry_time_str\", \"is_red\"]], on=[\"date\", \"entry_time_str\"], how=\"left\")\n",
    "    \n",
    "            if \"is_red\" in exit_df.columns:\n",
    "                cols = exit_df.columns.tolist()\n",
    "                cols.insert(cols.index(\"bsr\") + 1, cols.pop(cols.index(\"is_red\")))\n",
    "                exit_df = exit_df[cols]\n",
    "    \n",
    "            exit_df[\"entry\"] = pd.to_datetime(exit_df[\"entry_time\"]).dt.strftime(\"%H:%M\")\n",
    "            exit_df[\"exit\"] = pd.to_datetime(exit_df[\"exit_time\"]).dt.strftime(\"%H:%M\")\n",
    "    \n",
    "            final_cols = [\n",
    "                \"date\", \"entry\", \"exit\", \"type\", \"side\", \"hold\",\n",
    "                \"long_tp\", \"long_sl\", \"short_tp\", \"short_sl\",\n",
    "                \"bsr\", \"is_red\", \"vol%\", \"max\", \"top\", \"avg\", \"bot\", \"min\", \"amplitude\",\n",
    "                \"tp_pts\", \"sl_pts\", \"pnl\", \"cumpnl\"\n",
    "            ]\n",
    "            exit_df = exit_df[final_cols]\n",
    "    \n",
    "            for col in [\"max\", \"top\", \"avg\", \"bot\", \"min\", \"amplitude\"]:\n",
    "                exit_df[col] = exit_df[col].round(0).astype(\"Int32\")\n",
    "            for col in [\"long_tp\", \"long_sl\", \"short_tp\", \"short_sl\"]:\n",
    "                exit_df[col] = exit_df[col].dropna().round(0).astype(\"Int32\")\n",
    "    \n",
    "            final_cumpnl = exit_df[\"cumpnl\"].iloc[-1] if not exit_df.empty else 0\n",
    "            chunk_results.append({\n",
    "                **params,\n",
    "                'trade_count': len(exit_df),\n",
    "                'final_cumpnl': final_cumpnl\n",
    "            })\n",
    "    \n",
    "            del exit_df, range_df, red_df, exit_records\n",
    "            gc.collect()\n",
    "    \n",
    "        if chunk_results:\n",
    "            chunk_results_df = pd.DataFrame(chunk_results)\n",
    "            chunk_results_df.to_csv(\n",
    "                output_path,\n",
    "                mode=\"a\",\n",
    "                header=not header_written,\n",
    "                index=False\n",
    "            )\n",
    "            header_written = True\n",
    "            print(f\"塊 {chunk_start // CHUNK_SIZE + 1} 的結果已儲存至 {output_path}\")\n",
    "    \n",
    "        del chunk_results, chunk_results_df\n",
    "        gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# 8. 輸出最終 GridSearch 結果\n",
    "# =============================================================================\n",
    "if os.path.exists(output_path):\n",
    "    grid_results_df = pd.read_csv(output_path)\n",
    "    if not grid_results_df.empty:\n",
    "        best_params = grid_results_df.loc[grid_results_df['final_cumpnl'].idxmax()]\n",
    "        print(\"最佳參數組合：\")\n",
    "        print(best_params)\n",
    "        print(\"\\n所有參數組合結果：\")\n",
    "        print(grid_results_df.sort_values(by='final_cumpnl', ascending=False))\n",
    "        print(f\"\\nGridSearch 結果已儲存至 output_path\")\n",
    "    else:\n",
    "        print(\"無任何交易訊號生成。\")\n",
    "    del grid_results_df\n",
    "else:\n",
    "    print(\"無任何交易訊號生成，輸出檔案未生成。\")\n",
    "\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Shioaji]",
   "language": "python",
   "name": "shioaji"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
